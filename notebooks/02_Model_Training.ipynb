{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN7A1ZKj5nMKZH7SA9MmMb3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📓 Notebook 01: Exploratory Data Analysis\n",
        "\n",
        "## Short description of the notebook"
      ],
      "metadata": {
        "id": "IofWeS66Gejk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOTES\n",
        "What we have done:\n",
        "\n",
        "- Preliminary data inspection\n",
        "\n",
        "- Add missing data to LLL1\n",
        "  - Several classic methods tried with low accuracy\n",
        "  - Imputation with Iterative imputer is good\n",
        "  - Cannot find anything like LLL1 to extract the real missing data\n",
        "\n",
        "- Save clean dataset\n",
        "\n",
        "- Plots\n",
        "  - timeseries\n",
        "  - ACF, PACF\n",
        "  - Correlation matrix\n",
        "  - Rolling volatility\n",
        "  - ..."
      ],
      "metadata": {
        "id": "GuZKA1hXwslv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Dependencies"
      ],
      "metadata": {
        "id": "flR6Oc9bQzIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import random\n",
        "import logging\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "import re\n",
        "import joblib\n",
        "\n",
        "# Core scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Others\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from typing import List, Optional, Union\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "!pip install sktime\n",
        "from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
        "from typing import Generator, Tuple\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "EzPX8Gdja4DB",
        "outputId": "ff0dc323-9ca1-4ac0-e421-c9c171c544a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sktime in /usr/local/lib/python3.11/dist-packages (0.37.0)\n",
            "Requirement already satisfied: joblib<1.5,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.4.2)\n",
            "Requirement already satisfied: numpy<2.3,>=1.21 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from sktime) (24.2)\n",
            "Requirement already satisfied: pandas<2.3.0,>=1.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (2.2.2)\n",
            "Requirement already satisfied: scikit-base<0.13.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sktime) (0.12.3)\n",
            "Requirement already satisfied: scikit-learn<1.7.0,>=0.24 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.6.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.2 in /usr/local/lib/python3.11/dist-packages (from sktime) (1.15.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=0.24->sktime) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌐 General Setup"
      ],
      "metadata": {
        "id": "GakLOwrRHh29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FuWiaKoGG3vm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /content\n",
        "\n",
        "REPO=https://github.com/jacopo-raffaeli/portfolio-replica.git\n",
        "DIR=portfolio-replica\n",
        "\n",
        "# Clone if needed, else pull latest\n",
        "if [ ! -d \"$DIR\" ]; then\n",
        "  git clone $REPO > /dev/null 2>&1\n",
        "else\n",
        "  cd $DIR\n",
        "  git pull origin main > /dev/null 2>&1\n",
        "  cd ..\n",
        "fi\n",
        "\n",
        "# Enter project root and install dependencies\n",
        "cd $DIR\n",
        "pip install -r requirements.txt > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add repo root to sys.path for imports\n",
        "PROJECT_ROOT = \"/content/portfolio-replica\"\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "    sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n",
        "\n",
        "# Set working directory for relative paths\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypms8AzYXuZ6",
        "outputId": "eade039b-535d-4f6a-a316-c38edb5783f6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/portfolio-replica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Pandas display options\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.precision', 3)\n",
        "\n",
        "# Seaborn and Matplotlib display options\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "# Set reproducible seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s %(levelname)s %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define paths\n",
        "data_raw_path = \"data/raw/\"\n",
        "data_interim_path = \"data/interim/\"\n",
        "data_processed_path = \"data/processed/\""
      ],
      "metadata": {
        "id": "UkcNz5M4ZK8y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠 Utilities"
      ],
      "metadata": {
        "id": "bO3_aAynLI-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "_gtgmCgBGL51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_series(\n",
        "    df: pd.DataFrame,\n",
        "    cols: list,\n",
        "    title: str = None,\n",
        "    save_path: str = None,\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = True\n",
        "):\n",
        "    \"\"\"Plot one or more time series.\"\"\"\n",
        "    plt.figure()\n",
        "    ax = df[cols].plot()\n",
        "    ax.set_title(title or f\"Time Series: {', '.join(cols)}\")\n",
        "    ax.set_xlabel(\"Date\")\n",
        "    ax.set_ylabel(\"Value\")\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=dpi, transparent=transparent)\n",
        "\n",
        "def plot_acf_pacf(\n",
        "    series: pd.Series,\n",
        "    lags: int,\n",
        "    title_prefix: str = None,\n",
        "    save_path: str = None,\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = True\n",
        "):\n",
        "    \"\"\"Plot ACF and PACF of a series.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 1)\n",
        "    plot_acf(series.dropna(), lags=lags, ax=axes[0])\n",
        "    plot_pacf(series.dropna(), lags=lags, ax=axes[1])\n",
        "    axes[0].set_title((title_prefix or \"\") + \" ACF\")\n",
        "    axes[1].set_title((title_prefix or \"\") + \" PACF\")\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        fig.savefig(save_path, dpi=dpi, transparent=transparent)\n",
        "\n",
        "def plot_rolling_stats(\n",
        "    series: pd.Series,\n",
        "    window: int = 20,\n",
        "    zscore: bool = False,\n",
        "    title: str = None,\n",
        "    save_path: str = None,\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = True\n",
        "):\n",
        "    \"\"\"Plot rolling mean & std or z-score.\"\"\"\n",
        "    rolling_mean = series.rolling(window).mean()\n",
        "    rolling_std = series.rolling(window).std()\n",
        "\n",
        "    plt.figure()\n",
        "    ax = plt.gca()\n",
        "    if zscore:\n",
        "        z = (series - rolling_mean) / rolling_std\n",
        "        z.plot(ax=ax)\n",
        "        ax.axhline(0, linestyle=\"--\")\n",
        "        ax.axhline(1, linestyle=\"--\")\n",
        "        ax.axhline(-1, linestyle=\"--\")\n",
        "        ax.set_title(title or f\"Rolling Z-Score (window={window})\")\n",
        "    else:\n",
        "        series.plot(ax=ax, alpha=0.5, label=\"Original\")\n",
        "        rolling_mean.plot(ax=ax, label=f\"Mean ({window})\")\n",
        "        rolling_std.plot(ax=ax, label=f\"Std ({window})\")\n",
        "        ax.set_title(title or f\"Rolling Mean & Std (window={window})\")\n",
        "        ax.legend()\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=dpi, transparent=transparent)\n",
        "\n",
        "def plot_correlation_matrix(\n",
        "    df: pd.DataFrame,\n",
        "    method: str = \"pearson\",\n",
        "    title: str = None,\n",
        "    save_path: str = None,\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = True\n",
        "):\n",
        "    \"\"\"Plot correlation heatmap.\"\"\"\n",
        "    corr = df.corr(method=method)\n",
        "    plt.figure()\n",
        "    ax = sns.heatmap(corr, annot=True, fmt=\".2f\", square=True)\n",
        "    ax.set_title(title or f\"{method.title()} Correlation Matrix\")\n",
        "    plt.tight_layout()\n",
        "    if save_path:\n",
        "        Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n",
        "        plt.savefig(save_path, dpi=dpi, transparent=transparent)\n",
        "\n",
        "def plot_and_save(\n",
        "    plot_func,\n",
        "    *args,\n",
        "    plot_type: str,\n",
        "    subject: str,\n",
        "    variant: str = \"\",\n",
        "    directory: str = \"reports/figures\",\n",
        "    extension: str = \"png\",\n",
        "    dpi: int = 300,\n",
        "    transparent: bool = True,\n",
        "    show: bool = True,\n",
        "    **kwargs\n",
        "):\n",
        "    \"\"\"Wrapper to standardize plot saving and naming.\"\"\"\n",
        "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
        "    subject_clean = sanitize_filename(subject)\n",
        "    variant_clean = sanitize_filename(variant) if variant else \"\"\n",
        "    filename_parts = [plot_type, subject_clean]\n",
        "    if variant_clean:\n",
        "        filename_parts.append(variant_clean)\n",
        "    filename = \"__\".join(filename_parts) + f\".{extension}\"\n",
        "    save_path = Path(directory) / filename\n",
        "\n",
        "    plot_func(*args, save_path=str(save_path), dpi=dpi, transparent=transparent, **kwargs)\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def sanitize_filename(s: str) -> str:\n",
        "    \"\"\"Sanitize strings for filenames (e.g. column names).\"\"\"\n",
        "    return re.sub(r\"[^a-zA-Z0-9_]\", \"_\", s.strip().replace(\" \", \"_\"))"
      ],
      "metadata": {
        "id": "oVd9nezwE_cj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Prepare data"
      ],
      "metadata": {
        "id": "Y8VS4kUhopGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_index_returns(\n",
        "    df_idx: pd.DataFrame,\n",
        "    index_weights: Dict[str, float] = None\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Compute weighted composite index returns from raw index price levels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_idx : pd.DataFrame\n",
        "        DataFrame of index price levels (columns are index names).\n",
        "    index_weights : dict, optional\n",
        "        Mapping from index column to weight. If None, uses equal weights.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : pd.Series\n",
        "        Composite index returns.\n",
        "    \"\"\"\n",
        "    # Determine which indices to use\n",
        "    cols = df_idx.columns.tolist()\n",
        "    if index_weights is None:\n",
        "        # Equal weights if not provided\n",
        "        index_weights = {col: 1.0 / len(cols) for col in cols}\n",
        "    else:\n",
        "        # Validate provided keys\n",
        "        missing = set(index_weights) - set(cols)\n",
        "        if missing:\n",
        "            raise KeyError(f\"Index weights refer to unknown columns: {missing}\")\n",
        "\n",
        "    # Compute simple returns\n",
        "    ret = df_idx[list(index_weights.keys())].pct_change().dropna()\n",
        "\n",
        "    # Apply weights\n",
        "    weighted = pd.DataFrame({col: ret[col] * weight\n",
        "                             for col, weight in index_weights.items()},\n",
        "                            index=ret.index)\n",
        "\n",
        "    # Sum to get composite\n",
        "    y = weighted.sum(axis=1)\n",
        "    y.name = \"Target_Index\"\n",
        "    return y\n",
        "\n",
        "def compute_futures_returns(\n",
        "    df_fut: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute returns for futures price levels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_fut : pd.DataFrame\n",
        "        DataFrame of futures price levels (columns are futures names).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : pd.DataFrame\n",
        "        Futures returns DataFrame.\n",
        "    \"\"\"\n",
        "    X = df_fut.pct_change().dropna()\n",
        "    return X\n",
        "\n",
        "def align_features_target(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Align feature and target on common datetime index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "        Feature returns with datetime index.\n",
        "    y : pd.Series\n",
        "        Target returns with datetime index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_aligned : pd.DataFrame\n",
        "    y_aligned : pd.Series\n",
        "        Subsets of X and y sharing the same index.\n",
        "    \"\"\"\n",
        "    common_idx = X.index.intersection(y.index)\n",
        "    X_aligned = X.loc[common_idx]\n",
        "    y_aligned = y.loc[common_idx]\n",
        "    return X_aligned, y_aligned\n",
        "\n",
        "def prepare_X_y(\n",
        "    df_indices: pd.DataFrame,\n",
        "    df_futures: pd.DataFrame,\n",
        "    index_weights: Dict[str, float] = None\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    High-level wrapper: compute and align index and futures returns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_indices : pd.DataFrame\n",
        "        Raw index price levels.\n",
        "    df_futures : pd.DataFrame\n",
        "        Raw futures price levels.\n",
        "    index_weights : dict, optional\n",
        "        Composite index weights. Default: equal weights.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X, y : aligned returns ready for modeling\n",
        "    \"\"\"\n",
        "    y = compute_index_returns(df_indices, index_weights=index_weights)\n",
        "    X = compute_futures_returns(df_futures)\n",
        "    X_aligned, y_aligned = align_features_target(X, y)\n",
        "    return X_aligned, y_aligned\n",
        ""
      ],
      "metadata": {
        "id": "zw8c4d5krhu4"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "bisVmYFLu_n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_sliding_window_splits(\n",
        "    y: pd.Series,\n",
        "    window_length: int,\n",
        "    step_length: int = 1,\n",
        "    fh: int = 1\n",
        ") -> Generator[Tuple[pd.Index, pd.Index], None, None]:\n",
        "    \"\"\"\n",
        "    Yield train/test indices for a rolling (sliding) window over y.\n",
        "    Each fold trains on `window_length` points and tests on the next `fh` points,\n",
        "    then slides forward by `step_length`.\n",
        "    \"\"\"\n",
        "    splitter = SlidingWindowSplitter(\n",
        "        window_length=window_length,\n",
        "        step_length=step_length,\n",
        "        fh=list(range(1, fh + 1))\n",
        "    )\n",
        "    for train_idx, test_idx in splitter.split(y):\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "def get_expanding_window_splits(\n",
        "    y: pd.Series,\n",
        "    initial_window: int,\n",
        "    step_length: int = 1,\n",
        "    fh: int = 1\n",
        ") -> Generator[Tuple[pd.Index, pd.Index], None, None]:\n",
        "    \"\"\"\n",
        "    Yield train/test indices for an expanding window (walk-forward) over y.\n",
        "    Each fold trains on all data up to the current point (initial_window + k*step_length)\n",
        "    and tests on the next `fh` points.\n",
        "    \"\"\"\n",
        "    splitter = ExpandingWindowSplitter(\n",
        "        initial_window=initial_window,\n",
        "        step_length=step_length,\n",
        "        fh=list(range(1, fh + 1))\n",
        "    )\n",
        "    for train_idx, test_idx in splitter.split(y):\n",
        "        yield train_idx, test_idx\n",
        "\n",
        "def generate_outer_splits(\n",
        "    y: pd.Series,\n",
        "    strategy: str = \"sliding\",\n",
        "    window_length: int = 52,\n",
        "    step_length: int = 1,\n",
        "    fh: int = 1\n",
        ") -> Generator[Tuple[pd.Index, pd.Index], None, None]:\n",
        "    \"\"\"\n",
        "    Wrapper for sliding vs. expanding outer splits.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : pd.Series\n",
        "        Target series (used only for its index/length).\n",
        "    strategy : {'sliding', 'expanding'}\n",
        "        'sliding' -> fixed-size rolling window;\n",
        "        'expanding' -> growing walk-forward window.\n",
        "    window_length : int\n",
        "        Train window size (or initial size for expanding).\n",
        "    step_length : int\n",
        "        Number of periods to move forward each fold.\n",
        "    fh : int\n",
        "        Forecast horizon (test window size).\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    train_idx, test_idx : two numpy arrays of integer positions\n",
        "    \"\"\"\n",
        "    if strategy == \"sliding\":\n",
        "        yield from get_sliding_window_splits(y, window_length, step_length, fh)\n",
        "    elif strategy == \"expanding\":\n",
        "        yield from get_expanding_window_splits(y, window_length, step_length, fh)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy '{strategy}'\")\n"
      ],
      "metadata": {
        "id": "BHeylXb1vBhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 💾 Prepare Dataset"
      ],
      "metadata": {
        "id": "DCj_pgaBdBzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset & retrieve X, y\n",
        "indices_filepath = data_processed_path + \"indices.parquet\"\n",
        "futures_filepath = data_processed_path + \"futures_cleaned_imp_LLL1.parquet\"\n",
        "\n",
        "df_indeces = pd.read_parquet(indices_filepath)\n",
        "df_futures = pd.read_parquet(futures_filepath)\n",
        "\n",
        "index_weights = {\n",
        "    'HFRXGL': 0.25,\n",
        "    'MXWO':   0.25,\n",
        "    'MXWD':   0.25,\n",
        "    'LEGATRUU': 0.25\n",
        "}\n",
        "\n",
        "X, y = prepare_X_y(df_indeces, df_futures)\n"
      ],
      "metadata": {
        "id": "4DmBiIJudFxH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Visualize the monster index and its relations with the futures"
      ],
      "metadata": {
        "id": "ModbOfDHs1sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUSH"
      ],
      "metadata": {
        "id": "dlJaNuXjr0Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"jacoporaffaeli@gmail.com\"\n",
        "!git config --global user.name \"jacopo-raffaeli\""
      ],
      "metadata": {
        "id": "cwXuXNZKwm6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# 1. Get your GitHub Personal Access Token securely\n",
        "token = getpass(\"Paste your GitHub Personal Access Token: \")\n",
        "os.environ['GITHUB_TOKEN'] = token\n",
        "\n",
        "# 2. Set your GitHub repo details\n",
        "GITHUB_USERNAME = \"jacopo-raffaeli\"\n",
        "REPO_NAME = \"portfolio-replica\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# 3. Construct remote URL with token embedded (hidden from output)\n",
        "remote_url = f\"https://{token}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "\n",
        "# 4. Set git user info (if not done already)\n",
        "!git config --global user.email \"jacoporaffaeli@gmail.com\"\n",
        "!git config --global user.name \"jacopo-raffaeli\"\n",
        "\n",
        "# 5. Change remote origin URL to token-embedded one\n",
        "!git remote set-url origin {remote_url}\n"
      ],
      "metadata": {
        "id": "5mN7GvtVu3Kx",
        "outputId": "55434ae5-057b-4ffe-dfa3-eaab41e720c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your GitHub Personal Access Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Add and commit changes (customize your commit message)\n",
        "!git add .\n",
        "!git commit -m \"feat: Add Initial EDA\" || echo \"No changes to commit.\"\n",
        "\n",
        "# 7. Push to GitHub\n",
        "!git push origin {BRANCH}"
      ],
      "metadata": {
        "id": "Cr23drSAwENv",
        "outputId": "b8b9934d-1142-491d-a2ae-7dc62636602f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "No changes to commit.\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show git status, which files are changed and staged\n",
        "!git status\n",
        "\n",
        "# Show last commit files changed (to check if notebook was included)\n",
        "!git show --name-only --oneline -1\n",
        "\n",
        "# Show current branch\n",
        "!git branch"
      ],
      "metadata": {
        "id": "RgsoqJNs6-57",
        "outputId": "bf0de325-d174-49e3-8ca1-1473de76289f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "\u001b[33mf00faf2\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m Created using Colab\n",
            "notebooks/01_Exploratory_Data_Analysis.ipynb\n",
            "* \u001b[32mmain\u001b[m\n"
          ]
        }
      ]
    }
  ]
}