{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNPyHQp89rTGSf9kLJgBJKo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 📓 Notebook 02: Model Training\n",
        "\n",
        "## Short description of the notebook"
      ],
      "metadata": {
        "id": "IofWeS66Gejk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 📚 Dependencies"
      ],
      "metadata": {
        "id": "flR6Oc9bQzIC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install missing packages (Colab only)\n",
        "!pip install -q sktime pykalman\n"
      ],
      "metadata": {
        "id": "EzPX8Gdja4DB"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Standard Library ===\n",
        "import os\n",
        "import sys\n",
        "import warnings\n",
        "import random\n",
        "import logging\n",
        "import pickle\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Optional, Union, Dict, Tuple, Generator\n",
        "from dataclasses import dataclass, field\n",
        "\n",
        "# === Third-Party Libraries ===\n",
        "\n",
        "## Scientific stack\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "## Scikit-learn\n",
        "from sklearn.base import clone\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet, Lasso, BayesianRidge\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "\n",
        "## Statsmodels\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "## sktime (for time series splits)\n",
        "from sktime.forecasting.model_selection import SlidingWindowSplitter, ExpandingWindowSplitter\n",
        "\n",
        "## Kalman filter\n",
        "from pykalman import KalmanFilter\n",
        "\n",
        "# === Serialization ===\n",
        "import joblib\n"
      ],
      "metadata": {
        "id": "OIQRdvKcCFIe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🌐 General Setup"
      ],
      "metadata": {
        "id": "GakLOwrRHh29"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "FuWiaKoGG3vm",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "cd /content\n",
        "\n",
        "REPO=https://github.com/jacopo-raffaeli/portfolio-replica.git\n",
        "DIR=portfolio-replica\n",
        "\n",
        "# Clone if needed, else pull latest\n",
        "if [ ! -d \"$DIR\" ]; then\n",
        "  git clone $REPO > /dev/null 2>&1\n",
        "else\n",
        "  cd $DIR\n",
        "  git pull origin main > /dev/null 2>&1\n",
        "  cd ..\n",
        "fi\n",
        "\n",
        "# Enter project root and install dependencies\n",
        "cd $DIR\n",
        "pip install -r requirements.txt > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add repo root to sys.path for imports\n",
        "PROJECT_ROOT = \"/content/portfolio-replica\"\n",
        "if PROJECT_ROOT not in sys.path:\n",
        "    sys.path.append(PROJECT_ROOT)\n",
        "    sys.path.append(os.path.join(PROJECT_ROOT, 'src'))\n",
        "\n",
        "# Set working directory for relative paths\n",
        "os.chdir(PROJECT_ROOT)\n",
        "print(f\"Current working directory: {os.getcwd()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ypms8AzYXuZ6",
        "outputId": "c7fcad92-e8be-4dd0-d264-e28e3f2533d5"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /content/portfolio-replica\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Pandas display options\n",
        "pd.set_option('display.max_columns', 100)\n",
        "pd.set_option('display.precision', 3)\n",
        "\n",
        "# Seaborn and Matplotlib display options\n",
        "sns.set(style='whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "# Set reproducible seeds\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s %(levelname)s %(message)s',\n",
        "    datefmt='%H:%M:%S'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define paths\n",
        "data_raw_path = \"data/raw/\"\n",
        "data_interim_path = \"data/interim/\"\n",
        "data_processed_path = \"data/processed/\""
      ],
      "metadata": {
        "id": "UkcNz5M4ZK8y"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🛠 Utilities"
      ],
      "metadata": {
        "id": "bO3_aAynLI-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load & Prepare data"
      ],
      "metadata": {
        "id": "Y8VS4kUhopGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_index_returns(\n",
        "    df_idx: pd.DataFrame,\n",
        "    index_weights: Dict[str, float] = None\n",
        ") -> pd.Series:\n",
        "    \"\"\"\n",
        "    Compute weighted composite index returns from raw index price levels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_idx : pd.DataFrame\n",
        "        DataFrame of index price levels (columns are index names).\n",
        "    index_weights : dict, optional\n",
        "        Mapping from index column to weight. If None, uses equal weights.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    y : pd.Series\n",
        "        Composite index returns.\n",
        "    \"\"\"\n",
        "    # Determine which indices to use\n",
        "    cols = df_idx.columns.tolist()\n",
        "    if index_weights is None:\n",
        "        # Equal weights if not provided\n",
        "        index_weights = {col: 1.0 / len(cols) for col in cols}\n",
        "    else:\n",
        "        # Validate provided keys\n",
        "        missing = set(index_weights) - set(cols)\n",
        "        if missing:\n",
        "            raise KeyError(f\"Index weights refer to unknown columns: {missing}\")\n",
        "\n",
        "    # Compute simple returns\n",
        "    ret = df_idx[list(index_weights.keys())].pct_change(fill_method=None).dropna()\n",
        "\n",
        "    # Apply weights\n",
        "    weighted = pd.DataFrame({col: ret[col] * weight\n",
        "                             for col, weight in index_weights.items()},\n",
        "                            index=ret.index)\n",
        "\n",
        "    # Sum to get composite\n",
        "    y = weighted.sum(axis=1)\n",
        "    y.name = \"Target_Index\"\n",
        "    return y\n",
        "\n",
        "def compute_futures_returns(\n",
        "    df_fut: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute returns for futures price levels.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_fut : pd.DataFrame\n",
        "        DataFrame of futures price levels (columns are futures names).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X : pd.DataFrame\n",
        "        Futures returns DataFrame.\n",
        "    \"\"\"\n",
        "    X = df_fut.pct_change(fill_method=None).dropna()\n",
        "    return X\n",
        "\n",
        "def align_features_target(\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    Align feature and target on common datetime index.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : pd.DataFrame\n",
        "        Feature returns with datetime index.\n",
        "    y : pd.Series\n",
        "        Target returns with datetime index.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X_aligned : pd.DataFrame\n",
        "    y_aligned : pd.Series\n",
        "        Subsets of X and y sharing the same index.\n",
        "    \"\"\"\n",
        "    common_idx = X.index.intersection(y.index)\n",
        "    X_aligned = X.loc[common_idx]\n",
        "    y_aligned = y.loc[common_idx]\n",
        "    return X_aligned, y_aligned\n",
        "\n",
        "def prepare_X_y(\n",
        "    df_indices: pd.DataFrame,\n",
        "    df_futures: pd.DataFrame,\n",
        "    index_weights: Dict[str, float] = None\n",
        ") -> Tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"\n",
        "    High-level wrapper: compute and align index and futures returns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df_indices : pd.DataFrame\n",
        "        Raw index price levels.\n",
        "    df_futures : pd.DataFrame\n",
        "        Raw futures price levels.\n",
        "    index_weights : dict, optional\n",
        "        Composite index weights. Default: equal weights.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    X, y : aligned returns ready for modeling\n",
        "    \"\"\"\n",
        "    y = compute_index_returns(df_indices, index_weights=index_weights)\n",
        "    X = compute_futures_returns(df_futures)\n",
        "    X_aligned, y_aligned = align_features_target(X, y)\n",
        "    return X_aligned, y_aligned\n"
      ],
      "metadata": {
        "id": "zw8c4d5krhu4"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data"
      ],
      "metadata": {
        "id": "bisVmYFLu_n1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_backtest_splits(\n",
        "    y: pd.Series,\n",
        "    strategy: str = \"sliding\",\n",
        "    window_length: int = 52,\n",
        "    step_length: int = 1\n",
        ") -> Generator[Tuple[pd.Index, pd.Index], None, None]:\n",
        "    \"\"\"\n",
        "    One-step-ahead backtest splits for replication.\n",
        "\n",
        "    At each fold:\n",
        "      - Train on `window_length` points (or all past for expanding)\n",
        "      - Test on the single point immediately after that window (t+1)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    y : pd.Series\n",
        "        Target series (index only used for length).\n",
        "    strategy : {'sliding', 'expanding'}\n",
        "        'sliding'   → fixed-size rolling window of length `window_length`.\n",
        "        'expanding' → growing window that starts at size `window_length` and then increases.\n",
        "    window_length : int\n",
        "        For 'sliding': # samples in each train window.\n",
        "        For 'expanding': # samples in initial train window.\n",
        "    step_length : int\n",
        "        How many periods to move forward between fits.\n",
        "\n",
        "    Yields\n",
        "    ------\n",
        "    train_idx : np.ndarray\n",
        "        Integer positions for training (length = window_length or growing).\n",
        "    test_idx : np.ndarray\n",
        "        Single-element array containing the one-step-ahead index.\n",
        "    \"\"\"\n",
        "    # always one-step ahead\n",
        "    fh = [1]\n",
        "    if strategy == \"sliding\":\n",
        "        splitter = SlidingWindowSplitter(\n",
        "            window_length=window_length,\n",
        "            step_length=step_length,\n",
        "            fh=fh\n",
        "        )\n",
        "    elif strategy == \"expanding\":\n",
        "        splitter = ExpandingWindowSplitter(\n",
        "            initial_window=window_length,\n",
        "            step_length=step_length,\n",
        "            fh=fh\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown strategy '{strategy}'\")\n",
        "\n",
        "    for train_idx, test_idx in splitter.split(y):\n",
        "        yield train_idx, test_idx\n"
      ],
      "metadata": {
        "id": "5L-nTblZ1j5t"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simulate Backtest"
      ],
      "metadata": {
        "id": "xXrMC4iene2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(X, y, train_idx, test_idx):\n",
        "    \"\"\"\n",
        "    Extract training and test sets for a single backtest iteration.\n",
        "\n",
        "    Returns:\n",
        "        X_train, y_train: training features and target\n",
        "        X_next, y_next: test features and target (next step)\n",
        "        date_next: date/index of test step\n",
        "    \"\"\"\n",
        "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_next = X.iloc[test_idx].values.flatten()\n",
        "    y_next = y.iloc[test_idx].iloc[0]\n",
        "    date_next = y.index[test_idx][0]\n",
        "    return X_train, y_train, X_next, y_next, date_next\n",
        ""
      ],
      "metadata": {
        "id": "RQc8Z0V4BwAJ"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(build_model, model_params, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Build and fit model pipeline on training data.\n",
        "    \"\"\"\n",
        "    model = build_model(**model_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model\n",
        ""
      ],
      "metadata": {
        "id": "-H7RGPFJBxSM"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unscale_weights(model):\n",
        "    \"\"\"\n",
        "    Extract model weights and unscale them if scaler present.\n",
        "\n",
        "    Returns:\n",
        "        original_weights: scaled-back portfolio weights\n",
        "        scale: scaling factors from scaler or ones\n",
        "    \"\"\"\n",
        "    scaler = model.named_steps.get('scaler', None)\n",
        "    reg = model.named_steps['regressor']\n",
        "    normalized_weights = reg.coef_.copy()\n",
        "    if scaler is not None and hasattr(scaler, 'scale_'):\n",
        "        scale = scaler.scale_\n",
        "        original_weights = normalized_weights / scale\n",
        "    else:\n",
        "        scale = np.ones_like(normalized_weights)\n",
        "        original_weights = normalized_weights.copy()\n",
        "    return original_weights, scale\n",
        ""
      ],
      "metadata": {
        "id": "YeYtoX0_ByjK"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_backtest_regression(\n",
        "    X, y, splitter, build_model, model_params,\n",
        "    constraint_funcs=None,\n",
        "    constraint_params=None,\n",
        "    transaction_cost_rate=0.0,\n",
        "    scaler_name=\"standard\",\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Run one-step-ahead replication backtest for regression models.\n",
        "\n",
        "    Returns BacktestResult with weights, returns, metrics, etc.\n",
        "    \"\"\"\n",
        "    constraint_params = constraint_params or {}\n",
        "\n",
        "    # Initialize records\n",
        "    scale_history = []\n",
        "    weights_history = []\n",
        "    gross_exposures = []\n",
        "    replica_returns_gross = []\n",
        "    replica_returns_net = []\n",
        "    target_returns = []\n",
        "    dates = []\n",
        "\n",
        "    # Initialize constraints history\n",
        "    constraints_history = {fn.__name__: [] for fn in constraint_funcs} if constraint_funcs else {}\n",
        "\n",
        "    for step_num, (train_idx, test_idx) in enumerate(splitter):\n",
        "        # Prepare data\n",
        "        X_train, y_train, X_next, y_next, date_next = prepare_data(X, y, train_idx, test_idx)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Step {step_num+1}: Training model on {len(train_idx)} samples — predicting for {date_next.date()}\")\n",
        "\n",
        "        # Train model\n",
        "        model = train_model(build_model, model_params, X_train, y_train)\n",
        "\n",
        "        # Get weights and scale factor\n",
        "        original_weights, scale = unscale_weights(model)\n",
        "\n",
        "        # Apply constraints\n",
        "        original_weights = apply_constraints(\n",
        "            original_weights,\n",
        "            constraint_funcs=constraint_funcs,\n",
        "            prev_weights=weights_history,\n",
        "            prev_returns=replica_returns_net,\n",
        "            constraints_history=constraints_history,\n",
        "            constraint_params=constraint_params\n",
        "        )\n",
        "\n",
        "        # Compute returns and exposure\n",
        "        replica_return_gross, gross_exposure = compute_step_metrics(X_next, original_weights)\n",
        "\n",
        "        # Transaction cost\n",
        "        prev_w = weights_history[-1] if weights_history else None\n",
        "        transaction_cost = compute_transaction_cost(\n",
        "            current_weights=original_weights,\n",
        "            previous_weights=prev_w,\n",
        "            cost_rate=transaction_cost_rate\n",
        "        )\n",
        "\n",
        "        replica_return_net = replica_return_gross - transaction_cost\n",
        "\n",
        "        # Record everything\n",
        "        scale_history.append(scale)\n",
        "        weights_history.append(original_weights)\n",
        "        gross_exposures.append(gross_exposure)\n",
        "        replica_returns_gross.append(replica_return_gross)\n",
        "        replica_returns_net.append(replica_return_net)\n",
        "        target_returns.append(y_next)\n",
        "        dates.append(date_next)\n",
        "\n",
        "    # Final series\n",
        "    weights_history = np.vstack(weights_history)\n",
        "    replica_returns_gross = pd.Series(replica_returns_gross, index=dates, name='replica_returns')\n",
        "    replica_returns_net = pd.Series(replica_returns_net, index=dates, name='replica_returns')\n",
        "    target_returns = pd.Series(target_returns, index=dates, name='target_returns')\n",
        "\n",
        "    # Aggregate metrics\n",
        "    aggregate_metrics = compute_aggregate_metrics(\n",
        "        target_returns=target_returns,\n",
        "        replica_returns=replica_returns_net,\n",
        "        gross_exposures=gross_exposures,\n",
        "        scaling_factors=scale_history,\n",
        "        weights_history=weights_history,\n",
        "        config_params=model_params,\n",
        "    )\n",
        "\n",
        "    return BacktestResult(\n",
        "        model_name=model.__class__.__name__,\n",
        "        model_params=model_params,\n",
        "        weights_history=weights_history,\n",
        "        gross_exposures=gross_exposures,\n",
        "        scale_history=scale_history,\n",
        "        replica_returns=replica_returns_net,\n",
        "        target_returns=target_returns,\n",
        "        aggregate_metrics=aggregate_metrics,\n",
        "        constraints_history=constraints_history\n",
        "    )\n"
      ],
      "metadata": {
        "id": "IAa5pIy6u2eo"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simulate_backtest_kalman(\n",
        "    X, y, splitter, model_params,\n",
        "    constraint_funcs=None,\n",
        "    constraint_params=None,\n",
        "    transaction_cost_rate=0.0,\n",
        "    verbose=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Run one-step-ahead replication backtest using a Kalman filter\n",
        "    for dynamic weight estimation.\n",
        "    \"\"\"\n",
        "    constraint_params = constraint_params or {}\n",
        "\n",
        "    # Initialize histories\n",
        "    scale_history = []\n",
        "    weights_history = []\n",
        "    gross_exposures = []\n",
        "    replica_returns_gross = []\n",
        "    replica_returns_net = []\n",
        "    target_returns = []\n",
        "    dates = []\n",
        "\n",
        "    # Initialize constraints history\n",
        "    constraints_history = {fn.__name__: [] for fn in constraint_funcs} if constraint_funcs else {}\n",
        "\n",
        "    # Extract dimensions\n",
        "    n_assets = X.shape[1]\n",
        "\n",
        "    # Kalman model parameters\n",
        "    trans_cov  = model_params.get(\"trans_cov\", 1e-5)\n",
        "    obs_cov    = model_params.get(\"obs_cov\", 1e-2)\n",
        "    init_scale = model_params.get(\"init_scale\", 1e-3)\n",
        "\n",
        "    # Initial regressor parameters\n",
        "    init_with_regression = model_params.get(\"init_with_regression\", False)\n",
        "    regressor_cls = model_params.get(\"regressor_cls\", None)\n",
        "    regressor_params = model_params.get(\"regressor_params\", {})\n",
        "\n",
        "    # Initialize Kalman filter\n",
        "    kf = KalmanFilter(\n",
        "        transition_matrices      = np.eye(n_assets),\n",
        "        observation_matrices     = None,\n",
        "        transition_covariance    = trans_cov * np.eye(n_assets),\n",
        "        observation_covariance   = obs_cov,\n",
        "        initial_state_mean       = np.zeros(n_assets),\n",
        "        initial_state_covariance = init_scale * np.eye(n_assets)\n",
        "    )\n",
        "\n",
        "    # Initialize Kalman state\n",
        "    state_mean = np.zeros(n_assets)\n",
        "    state_cov  = init_scale * np.eye(n_assets)\n",
        "\n",
        "    for step_num, (train_idx, test_idx) in enumerate(splitter):\n",
        "        # Per-step data\n",
        "        X_train, y_train, X_next, y_next, date_next = prepare_data(X, y, train_idx, test_idx)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"Step {step_num+1}: Kalman updating on {date_next.date()}\")\n",
        "\n",
        "        # Use only last observation in train set to update\n",
        "        x_t = X_train.iloc[-1].values\n",
        "        y_t = float(y_train.iloc[-1])\n",
        "\n",
        "        # Weight initialization with regressor\n",
        "        if init_with_regression and step_num == 0:\n",
        "            if regressor_cls is None:\n",
        "                raise ValueError(\"To use init_with_regression=True, you must provide regressor_cls.\")\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"Initializing Kalman weights with {regressor_cls.__name__} on window ending {date_next.date()}\")\n",
        "\n",
        "            reg = regressor_cls(**(regressor_params or {}))\n",
        "            reg.fit(X_train, y_train)\n",
        "            init_weights = reg.coef_\n",
        "\n",
        "            # Override initial state mean\n",
        "            state_mean = init_weights.copy()\n",
        "\n",
        "        # Kalman update step\n",
        "        state_mean, state_cov = kf.filter_update(\n",
        "            filtered_state_mean       = state_mean,\n",
        "            filtered_state_covariance = state_cov,\n",
        "            observation               = y_t,\n",
        "            observation_matrix        = x_t[np.newaxis, :]\n",
        "        )\n",
        "\n",
        "        original_weights = state_mean.copy()\n",
        "        scale = None  # not applicable in Kalman\n",
        "\n",
        "        # Apply constraints\n",
        "        original_weights, constraints_metadata = apply_constraints(\n",
        "            original_weights,\n",
        "            constraint_funcs=constraint_funcs,\n",
        "            prev_weights=weights_history,\n",
        "            prev_returns=replica_returns_net,\n",
        "            constraints_history=constraints_history,\n",
        "            constraint_params=constraint_params\n",
        "        )\n",
        "\n",
        "        # Compute step metrics\n",
        "        replica_return_gross, gross_exposure = compute_step_metrics(X_next, original_weights)\n",
        "\n",
        "        prev_w = weights_history[-1] if weights_history else None\n",
        "        transaction_cost = compute_transaction_cost(\n",
        "            current_weights=original_weights,\n",
        "            previous_weights=prev_w,\n",
        "            cost_rate=transaction_cost_rate\n",
        "        )\n",
        "\n",
        "        replica_return_net = replica_return_gross - transaction_cost\n",
        "\n",
        "        # Record\n",
        "        scale_history.append(scale)\n",
        "        weights_history.append(original_weights)\n",
        "        gross_exposures.append(gross_exposure)\n",
        "        replica_returns_gross.append(replica_return_gross)\n",
        "        replica_returns_net.append(replica_return_net)\n",
        "        target_returns.append(y_next)\n",
        "        dates.append(date_next)\n",
        "\n",
        "    # Final time-series\n",
        "    weights_history = np.vstack(weights_history)\n",
        "    replica_returns_gross = pd.Series(replica_returns_gross, index=dates, name='replica_returns')\n",
        "    replica_returns_net = pd.Series(replica_returns_net, index=dates, name='replica_returns')\n",
        "    target_returns = pd.Series(target_returns, index=dates, name='target_returns')\n",
        "\n",
        "    # Aggregate metrics\n",
        "    aggregate_metrics = compute_aggregate_metrics(\n",
        "        target_returns=target_returns,\n",
        "        replica_returns=replica_returns_net,\n",
        "        gross_exposures=gross_exposures,\n",
        "        scaling_factors=scale_history,\n",
        "        weights_history=weights_history,\n",
        "        config_params=model_params,\n",
        "    )\n",
        "\n",
        "    return BacktestResult(\n",
        "        model_name=\"Kalman\",\n",
        "        model_params=model_params,\n",
        "        weights_history=weights_history,\n",
        "        gross_exposures=gross_exposures,\n",
        "        scale_history=scale_history,\n",
        "        replica_returns=replica_returns_net,\n",
        "        target_returns=target_returns,\n",
        "        aggregate_metrics=aggregate_metrics,\n",
        "        constraints_history=constraints_history\n",
        "    )\n"
      ],
      "metadata": {
        "id": "CdAhvdNS43Cf"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class BacktestResult:\n",
        "    model_name: str\n",
        "    model_params: dict\n",
        "    weights_history: np.ndarray\n",
        "    gross_exposures: list\n",
        "    scale_history: list\n",
        "    replica_returns: pd.Series\n",
        "    target_returns: pd.Series\n",
        "    aggregate_metrics: dict\n",
        "    constraints_history: dict\n",
        "\n",
        "    def summary(self):\n",
        "        return {\n",
        "            'model': self.model_name,\n",
        "            'sharpe': self.aggregate_metrics.get('replica_sharpe'),\n",
        "            'IR': self.aggregate_metrics.get('information_ratio'),\n",
        "            'TE': self.aggregate_metrics.get('tracking_error'),\n",
        "            'corr': self.aggregate_metrics.get('correlation')\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PdQRViZss_x0"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Constraints"
      ],
      "metadata": {
        "id": "qgMYcKAq76w2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_constraints(\n",
        "    original_weights,\n",
        "    constraint_funcs=None,\n",
        "    prev_weights=None,\n",
        "    prev_returns=None,\n",
        "    constraints_history=None,\n",
        "    constraint_params=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Apply each constraint function in order, modifying weights as needed.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    original_weights : np.ndarray\n",
        "        Proposed weights before constraints.\n",
        "    constraint_funcs : list of callables\n",
        "        Constraint functions, each taking (weights, weights_history, returns, **params).\n",
        "    prev_weights : list of np.ndarray\n",
        "        History of weights.\n",
        "    prev_returns : list of float\n",
        "        History of past returns.\n",
        "    constraints_history : dict\n",
        "        Dict to store metadata for each constraint.\n",
        "    constraint_params : dict\n",
        "        Dict mapping constraint function names to parameter dicts.\n",
        "    \"\"\"\n",
        "    constraint_params = constraint_params or {}\n",
        "    constraints_history = constraints_history or {}\n",
        "\n",
        "    if constraint_funcs:\n",
        "        for fn in constraint_funcs:\n",
        "            fn_name = fn.__name__\n",
        "            fn_params = constraint_params.get(fn_name, {})\n",
        "\n",
        "            # Call constraint function with injected params\n",
        "            original_weights, metadata = fn(\n",
        "                original_weights,\n",
        "                prev_weights,\n",
        "                prev_returns,\n",
        "                **fn_params\n",
        "            )\n",
        "\n",
        "            # Store metadata\n",
        "            if constraints_history is not None:\n",
        "                if fn_name not in constraints_history:\n",
        "                    constraints_history[fn_name] = []\n",
        "                constraints_history[fn_name].append(metadata)\n",
        "\n",
        "    return original_weights, constraints_history\n"
      ],
      "metadata": {
        "id": "G24ZC8Q-7_db"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def constraint_gross_exposure(weights, weights_history, replica_returns, max_gross=2):\n",
        "    gross = np.sum(np.abs(weights))\n",
        "    metadata = {\n",
        "        'activated': False,\n",
        "        'violation_amount': 0.0,\n",
        "        'rescaling_factor': 1.0,\n",
        "        'gross_exposure': gross\n",
        "    }\n",
        "    if gross > max_gross:\n",
        "        metadata['activated'] = True\n",
        "        metadata['violation_amount'] = gross - max_gross\n",
        "        rescale = max_gross / gross\n",
        "        weights = weights * rescale\n",
        "        metadata['rescaling_factor'] = rescale\n",
        "    return weights, metadata\n"
      ],
      "metadata": {
        "id": "tvLlA7efAx_u"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_var(returns, method: str, confidence: float = 0.01, horizon: int = 4) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Value at Risk (VaR) over a given time horizon.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : array-like\n",
        "        Historical returns (e.g. weekly P&L series).\n",
        "    method : {'gaussian', 'historical', 'cornish-fisher'}\n",
        "        VaR calculation method.\n",
        "    confidence : float\n",
        "        Tail probability (e.g. 0.01 for 1% VaR).\n",
        "    horizon : int\n",
        "        Time horizon (in same units as returns, e.g. weeks).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    var : float\n",
        "        Positive number representing the loss at the given confidence/day horizon.\n",
        "    \"\"\"\n",
        "    r = np.asarray(returns)\n",
        "    if method == 'gaussian':\n",
        "        mu, sigma = np.mean(r), np.std(r)\n",
        "        # z for one-sided quantile\n",
        "        z = abs(np.percentile(np.random.standard_normal(10_000), confidence * 100))\n",
        "        var = -(mu + z * sigma) * np.sqrt(horizon)\n",
        "    elif method == 'historical':\n",
        "        # empirical quantile (these returns are negative if losses)\n",
        "        hist_q = np.percentile(r, confidence * 100)\n",
        "        var = -hist_q * np.sqrt(horizon)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown VaR method '{method}'\")\n",
        "    return var\n",
        "\n",
        "def constraint_var_historical(\n",
        "    weights: np.ndarray,\n",
        "    weights_history: list,\n",
        "    replica_returns: list,\n",
        "    max_var: float = 0.2,\n",
        "    var_confidence: float = 0.01,\n",
        "    var_horizon: int = 4,\n",
        "    lookback: int = 20\n",
        ") -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Project weights to satisfy a maximum historical VaR constraint.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    weights : np.ndarray\n",
        "        Proposed portfolio weights.\n",
        "    weights_history : list of np.ndarray\n",
        "        Past weights (unused here).\n",
        "    replica_returns : list of float\n",
        "        Past one-step replica returns.\n",
        "    max_var : float\n",
        "        Maximum allowed VaR (positive number).\n",
        "    var_confidence : float\n",
        "        Tail probability for VaR (e.g. 0.01 for 1%).\n",
        "    var_horizon : int\n",
        "        Horizon over which to scale VaR (e.g. 4 weeks).\n",
        "    lookback : int\n",
        "        Number of past returns to use for historical simulation.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_weights, metadata : (np.ndarray, dict)\n",
        "        Possibly rescaled weights and a metadata dict describing the adjustment.\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'activated': False,\n",
        "        'estimated_var': np.nan,\n",
        "        'violation': 0.0,\n",
        "        'rescale_factor': 1.0\n",
        "    }\n",
        "    if len(replica_returns) < lookback:\n",
        "        # Not enough history to estimate VaR\n",
        "        return weights, metadata\n",
        "\n",
        "    hist = replica_returns[-lookback:]\n",
        "    est_var = calculate_var(hist, method='historical', confidence=var_confidence, horizon=var_horizon)\n",
        "    metadata['estimated_var'] = est_var\n",
        "\n",
        "    if est_var > max_var:\n",
        "        metadata['activated'] = True\n",
        "        metadata['violation'] = est_var - max_var\n",
        "        factor = max_var / est_var\n",
        "        weights = weights * factor\n",
        "        metadata['rescale_factor'] = factor\n",
        "\n",
        "    return weights, metadata\n"
      ],
      "metadata": {
        "id": "KoQ_BTiSDHkG"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def constraint_turnover_band(\n",
        "    weights: np.ndarray,\n",
        "    weights_history: list,\n",
        "    replica_returns: list,\n",
        "    min_turnover: float = 0.02,\n",
        "    max_turnover: float = 0.10\n",
        ") -> tuple[np.ndarray, dict]:\n",
        "    \"\"\"\n",
        "    Enforce both a minimum no-trade threshold and a maximum turnover cap.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    weights : np.ndarray\n",
        "        Proposed new weights.\n",
        "    weights_history : list of np.ndarray\n",
        "        Past weights; last entry is w_{t-1}.\n",
        "    replica_returns : list\n",
        "        Past returns (unused here).\n",
        "    min_turnover : float\n",
        "        Below this turnover, skip rebalancing entirely (no-trade band).\n",
        "    max_turnover : float\n",
        "        Above this turnover, scale changes down to this level.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    new_weights, metadata : (np.ndarray, dict)\n",
        "    \"\"\"\n",
        "    metadata = {\n",
        "        'activated_min': False,\n",
        "        'activated_max': False,\n",
        "        'turnover': 0.0,\n",
        "        'viol_min': 0.0,\n",
        "        'viol_max': 0.0,\n",
        "        'rescale_factor': 1.0\n",
        "    }\n",
        "    if not weights_history:\n",
        "        return weights, metadata\n",
        "\n",
        "    prev = weights_history[-1]\n",
        "    turnover = np.sum(np.abs(weights - prev))\n",
        "    metadata['turnover'] = turnover\n",
        "\n",
        "    # No-trade band\n",
        "    if turnover < min_turnover:\n",
        "        metadata['activated_min'] = True\n",
        "        metadata['viol_min'] = min_turnover - turnover\n",
        "        return prev.copy(), metadata  # stay at old weights\n",
        "\n",
        "    # Max-turnover cap\n",
        "    if turnover > max_turnover:\n",
        "        metadata['activated_max'] = True\n",
        "        metadata['viol_max'] = turnover - max_turnover\n",
        "        factor = max_turnover / turnover\n",
        "        weights = prev + factor * (weights - prev)\n",
        "        metadata['rescale_factor'] = factor\n",
        "\n",
        "    return weights, metadata\n"
      ],
      "metadata": {
        "id": "EVqRpz2EBzly"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transaction costs"
      ],
      "metadata": {
        "id": "DOTHdw9PHdSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_transaction_cost(\n",
        "    current_weights: np.ndarray,\n",
        "    previous_weights: Optional[np.ndarray] = None,\n",
        "    cost_rate: float = 0.0004\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Compute round‑trip transaction cost based on turnover.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    current_weights : np.ndarray\n",
        "        New portfolio weights w_t.\n",
        "    previous_weights : np.ndarray or None\n",
        "        Prior portfolio weights w_{t-1}. If None, assumed zero (initial alloc).\n",
        "    cost_rate : float\n",
        "        Round‑trip cost per unit turnover (e.g. 0.0004 for 4 bps).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cost : float\n",
        "        Transaction cost to be subtracted from portfolio return.\n",
        "    \"\"\"\n",
        "    if previous_weights is None:\n",
        "        prev = np.zeros_like(current_weights)\n",
        "    else:\n",
        "        prev = previous_weights\n",
        "    turnover = np.sum(np.abs(current_weights - prev))\n",
        "    return turnover * cost_rate\n"
      ],
      "metadata": {
        "id": "sKD4PXHPHcwV"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n"
      ],
      "metadata": {
        "id": "VXqZgGPfT0jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_step_metrics(X_next, original_weights):\n",
        "    \"\"\"\n",
        "    Compute per-step metrics.\n",
        "    \"\"\"\n",
        "    replica_return = float(np.dot(X_next, original_weights))\n",
        "    gross_exposure = np.sum(np.abs(original_weights))\n",
        "    return replica_return, gross_exposure"
      ],
      "metadata": {
        "id": "iZc7_Sv7B4FF"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_aggregate_metrics(\n",
        "    target_returns: pd.Series,\n",
        "    replica_returns: pd.Series,\n",
        "    gross_exposures: list,\n",
        "    scaling_factors: list,\n",
        "    weights_history: list,\n",
        "    config_params: dict\n",
        ") -> dict:\n",
        "    \"\"\"Compute overall evaluation metrics for backtest.\"\"\"\n",
        "    # Compute cumulative returns for both target and replica\n",
        "    cumulative_target = (1 + target_returns).cumprod()\n",
        "    cumulative_replica = (1 + replica_returns).cumprod()\n",
        "\n",
        "    # Annualized return and volatility\n",
        "    target_mean_return = target_returns.mean() * 52\n",
        "    replica_mean_return = replica_returns.mean() * 52\n",
        "    target_vol = target_returns.std() * np.sqrt(52)\n",
        "    replica_vol = replica_returns.std() * np.sqrt(52)\n",
        "\n",
        "    # Sharpe ratio\n",
        "    target_sharpe = target_mean_return / target_vol if target_vol > 0 else np.nan\n",
        "    replica_sharpe = replica_mean_return / replica_vol if replica_vol > 0 else np.nan\n",
        "\n",
        "    # Drawdons\n",
        "    target_drawdown = 1 - cumulative_target / cumulative_target.cummax()\n",
        "    replica_drawdown = 1 - cumulative_replica / cumulative_replica.cummax()\n",
        "\n",
        "    # TE, IR, Corr\n",
        "    tracking_error = (replica_returns - target_returns).std() * np.sqrt(52)\n",
        "    information_ratio = (replica_mean_return - target_mean_return) / tracking_error if tracking_error > 0 else np.nan\n",
        "    correlation = replica_returns.corr(target_returns)\n",
        "\n",
        "    return {\n",
        "        **config_params,\n",
        "        'target_returns': target_returns,\n",
        "        'replica_returns': replica_returns,\n",
        "        'cumulative_target': cumulative_target,\n",
        "        'cumulative_replica': cumulative_replica,\n",
        "        'target_mean_return': target_mean_return,\n",
        "        'replica_mean_return': replica_mean_return,\n",
        "        'target_vol': target_vol,\n",
        "        'replica_vol': replica_vol,\n",
        "        'target_sharpe': target_sharpe,\n",
        "        'replica_sharpe': replica_sharpe,\n",
        "        'target_max_drawdown': target_drawdown.max(),\n",
        "        'replica_max_drawdown': replica_drawdown.max(),\n",
        "        'tracking_error': tracking_error,\n",
        "        'information_ratio': information_ratio,\n",
        "        'correlation': correlation,\n",
        "        'gross_exposures': gross_exposures,\n",
        "        'avg_gross_exposure': np.mean(gross_exposures),\n",
        "        'scaling_factors': scaling_factors,\n",
        "        'weights_history': weights_history\n",
        "    }\n"
      ],
      "metadata": {
        "id": "b8MN9I44W132"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiments"
      ],
      "metadata": {
        "id": "hGJ7sajcyROn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(model_name: str, scaler_name: str = \"standard\", **params):\n",
        "    \"\"\"\n",
        "    Return a scikit-learn pipeline with a scaler and the selected regression model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_name : str\n",
        "        One of ['linear', 'ridge', 'lasso', 'elasticnet']\n",
        "    scaler_name : str\n",
        "        One of ['standard', 'minmax', 'none']\n",
        "    **params : keyword arguments passed to the regressor\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sklearn.pipeline.Pipeline\n",
        "    \"\"\"\n",
        "    # Select regressor\n",
        "    regressors = {\n",
        "        'linear': LinearRegression,\n",
        "        'ridge': Ridge,\n",
        "        'lasso': Lasso,\n",
        "        'elasticnet': ElasticNet\n",
        "    }\n",
        "\n",
        "    if model_name not in regressors:\n",
        "        raise ValueError(f\"Model '{model_name}' not supported.\")\n",
        "\n",
        "    regressor = regressors[model_name](**params)\n",
        "\n",
        "    # Select scaler\n",
        "    scalers = {\n",
        "        'standard': StandardScaler(),\n",
        "        'minmax': MinMaxScaler(),\n",
        "        'none': 'passthrough'\n",
        "    }\n",
        "\n",
        "    if scaler_name not in scalers:\n",
        "        raise ValueError(f\"Scaler '{scaler_name}' not supported.\")\n",
        "\n",
        "    scaler = scalers[scaler_name]\n",
        "\n",
        "    return Pipeline([\n",
        "        (\"scaler\", scaler),\n",
        "        (\"regressor\", regressor)\n",
        "    ])\n"
      ],
      "metadata": {
        "id": "pg0cHoi0ySvz"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_single_backtest_experiment(config: dict) -> BacktestResult:\n",
        "    \"\"\"\n",
        "    Run a backtest experiment from a unified configuration dictionary.\n",
        "\n",
        "    Supports both regression-based and Kalman filter-based models.\n",
        "\n",
        "    Expected keys in config:\n",
        "    ...\n",
        "    \"\"\"\n",
        "    model_name = config['model_name'].lower()\n",
        "\n",
        "    # Filter relevant parameters for the selected model\n",
        "    raw_model_params = config.get('model_params', {})\n",
        "    model_params = filter_model_params(model_name, raw_model_params, verbose=config.get(\"verbose\", False))\n",
        "\n",
        "    # Create backtest splits\n",
        "    splitter = generate_backtest_splits(\n",
        "        y=config['y'],\n",
        "        strategy=config.get('split_strategy', 'sliding'),\n",
        "        window_length=config['window'],\n",
        "        step_length=config['step']\n",
        "    )\n",
        "\n",
        "    # Kalman model\n",
        "    if model_name == 'kalman':\n",
        "        return simulate_backtest_kalman(\n",
        "            X=config['X'],\n",
        "            y=config['y'],\n",
        "            splitter=splitter,\n",
        "            model_params=model_params,\n",
        "            constraint_funcs=config.get('constraint_funcs', None),\n",
        "            constraint_params=config.get('constraint_params', {}),\n",
        "            transaction_cost_rate=config.get(\"transaction_cost_rate\", 0.0),\n",
        "            verbose=config.get(\"verbose\", False),\n",
        "        )\n",
        "\n",
        "    # Regressor model\n",
        "    build_model = lambda **p: get_model(model_name, scaler_name=config.get(\"scaler_name\", \"standard\"), **p)\n",
        "    return simulate_backtest_regression(\n",
        "        X=config['X'],\n",
        "        y=config['y'],\n",
        "        splitter=splitter,\n",
        "        build_model=build_model,\n",
        "        model_params=model_params,\n",
        "        constraint_funcs=config.get('constraint_funcs', None),\n",
        "        constraint_params=config.get(\"constraint_params\", {}),\n",
        "        transaction_cost_rate=config.get(\"transaction_cost_rate\", 0.0),\n",
        "        scaler_name=config.get(\"scaler_name\", \"standard\"),\n",
        "        verbose=config.get(\"verbose\", False),\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "e87sYMIn8NX9"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ALLOWED_MODEL_PARAMS = {\n",
        "    \"linear\":       [],\n",
        "    \"ridge\":        [\"alpha\"],\n",
        "    \"lasso\":        [\"alpha\"],\n",
        "    \"elasticnet\":   [\"alpha\", \"l1_ratio\"],\n",
        "    \"bayesianridge\":[\"alpha_1\", \"alpha_2\", \"lambda_1\", \"lambda_2\"],\n",
        "    \"kalman\":       [\"trans_cov\", \"obs_cov\", \"init_scale\"],\n",
        "}"
      ],
      "metadata": {
        "id": "aOQa7-1n9hqf"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_model_params(model_name: str, all_params: dict, verbose: bool = False) -> dict:\n",
        "    allowed_keys = ALLOWED_MODEL_PARAMS.get(model_name.lower(), [])\n",
        "    filtered = {k: v for k, v in all_params.items() if k in allowed_keys}\n",
        "    if verbose:\n",
        "        ignored = [k for k in all_params if k not in allowed_keys]\n",
        "        if ignored:\n",
        "            print(f\"[WARN] Ignored model parameters for {model_name}: {ignored}\")\n",
        "    return filtered"
      ],
      "metadata": {
        "id": "_Gp4d_zZ9nrD"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display results"
      ],
      "metadata": {
        "id": "0IapGqmbycVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "def display_backtest_result(result):\n",
        "    \"\"\"\n",
        "    Display aggregate metrics and plot cumulative returns for a backtest result.\n",
        "    \"\"\"\n",
        "    print(\"=== Aggregate Metrics ===\")\n",
        "    for k, v in result.aggregate_metrics.items():\n",
        "        if isinstance(v, (float, int)):\n",
        "            print(f\"{k}: {v:.4f}\")\n",
        "        elif isinstance(v, str):\n",
        "            print(f\"{k}: {v}\")\n",
        "        # Skip arrays or series here\n",
        "\n",
        "    cum_target = (1 + result.target_returns).cumprod()\n",
        "    cum_replica = (1 + result.replica_returns).cumprod()\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(cum_target, label=\"Target Index\")\n",
        "    plt.plot(cum_replica, label=\"Replica Portfolio\")\n",
        "    plt.title(\"Cumulative Returns — Backtest Result\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Growth of $1\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "mVb1db7zye10"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🧩 Prepare Training Data"
      ],
      "metadata": {
        "id": "DCj_pgaBdBzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset & retrieve X, y\n",
        "indices_filepath = data_processed_path + \"indices.parquet\"\n",
        "futures_filepath = data_processed_path + \"futures_cleaned_imp_LLL1.parquet\"\n",
        "df_indeces = pd.read_parquet(indices_filepath)\n",
        "df_futures = pd.read_parquet(futures_filepath)\n",
        "\n",
        "# Set weights for the monster index\n",
        "index_weights = {\n",
        "    'HFRXGL': 0.50,\n",
        "    'LEGATRUU': 0.25,\n",
        "    'MXWO':   0.25,\n",
        "    'MXWD':   0.\n",
        "}\n",
        "\n",
        "# Extract X, y for model training\n",
        "X, y = prepare_X_y(df_indeces, df_futures, index_weights)\n"
      ],
      "metadata": {
        "id": "4DmBiIJudFxH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Visualize the monster index and its relations with the futures"
      ],
      "metadata": {
        "id": "ModbOfDHs1sG"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ⚡ Experiments"
      ],
      "metadata": {
        "id": "hR4avnwWmVFJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "experiment_config = {\n",
        "    # === Model Selection ===\n",
        "    \"model_name\": \"elasticnet\",\n",
        "    \"model_params\": {\n",
        "\n",
        "        # Kalman weight init\n",
        "        \"init_with_regression\": True,\n",
        "        \"regressor_cls\": ElasticNet,\n",
        "        \"regressor_params\": {\n",
        "            \"alpha\": 0.01,\n",
        "            \"l1_ratio\": 0.0,\n",
        "        },\n",
        "\n",
        "        # Regressor-specific\n",
        "        \"alpha\":    0.01,\n",
        "        \"l1_ratio\": 0.0\n",
        "    },\n",
        "    # Kalman only, initial vector of weights\n",
        "    \"init_with_regression\": False,\n",
        "\n",
        "    # === Preprocessing ===\n",
        "    \"scaler_name\": \"standard\",\n",
        "\n",
        "    # === Dataset ===\n",
        "    \"X\": X,\n",
        "    \"y\": y,\n",
        "\n",
        "    # === Backtest Settings ===\n",
        "    \"split_strategy\": \"sliding\",\n",
        "    \"window\":          156,\n",
        "    \"step\":            1,\n",
        "\n",
        "    # === Constraints ===\n",
        "    \"constraint_funcs\": [\n",
        "        constraint_gross_exposure,\n",
        "        constraint_var_historical,\n",
        "        constraint_turnover_band\n",
        "    ],\n",
        "    \"constraint_params\": {\n",
        "        \"constraint_gross_exposure\": {},\n",
        "        \"constraint_var_historical\": {},\n",
        "        \"constraint_turnover_band\": {\"min_turnover\": 0.01, \"max_turnover\": 0.2}\n",
        "    },\n",
        "\n",
        "    # === Transaction Costs ===\n",
        "    \"transaction_cost_rate\": 4*1e-4,\n",
        "\n",
        "    # === Logging ===\n",
        "    \"verbose\":          False,\n",
        "    \"save_model\":       False,\n",
        "    \"model_save_path\":  None\n",
        "}\n",
        "\n",
        "result = run_single_backtest_experiment(experiment_config)\n",
        "display_backtest_result(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "SOZNVRpZmY8I",
        "outputId": "0debc85a-db71-4706-f8a4-cb124cf1dbf5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-70-2419297207.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m }\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_single_backtest_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mdisplay_backtest_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-63-368210113.py\u001b[0m in \u001b[0;36mrun_single_backtest_experiment\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Regressor model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mbuild_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scaler_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"standard\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     return simulate_backtest_regression(\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-52-224305204.py\u001b[0m in \u001b[0;36msimulate_backtest_regression\u001b[0;34m(X, y, splitter, build_model, model_params, constraint_funcs, constraint_params, transaction_cost_rate, scaler_name, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# Compute returns and exposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mreplica_return_gross\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgross_exposure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_step_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Transaction cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-60-3553107064.py\u001b[0m in \u001b[0;36mcompute_step_metrics\u001b[0;34m(X_next, original_weights)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mCompute\u001b[0m \u001b[0mper\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mreplica_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgross_exposure\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreplica_return\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgross_exposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST"
      ],
      "metadata": {
        "id": "DeryjtJvF6f4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSXChsEHGATX",
        "outputId": "127e47a1-4e38-4b08-dc77-3b8073584143"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(7.999999999999998e-05)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PUSH"
      ],
      "metadata": {
        "id": "dlJaNuXjr0Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"jacoporaffaeli@gmail.com\"\n",
        "!git config --global user.name \"jacopo-raffaeli\""
      ],
      "metadata": {
        "id": "cwXuXNZKwm6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "# 1. Get your GitHub Personal Access Token securely\n",
        "token = getpass(\"Paste your GitHub Personal Access Token: \")\n",
        "os.environ['GITHUB_TOKEN'] = token\n",
        "\n",
        "# 2. Set your GitHub repo details\n",
        "GITHUB_USERNAME = \"jacopo-raffaeli\"\n",
        "REPO_NAME = \"portfolio-replica\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "# 3. Construct remote URL with token embedded (hidden from output)\n",
        "remote_url = f\"https://{token}@github.com/{GITHUB_USERNAME}/{REPO_NAME}.git\"\n",
        "\n",
        "# 4. Set git user info (if not done already)\n",
        "!git config --global user.email \"jacoporaffaeli@gmail.com\"\n",
        "!git config --global user.name \"jacopo-raffaeli\"\n",
        "\n",
        "# 5. Change remote origin URL to token-embedded one\n",
        "!git remote set-url origin {remote_url}\n"
      ],
      "metadata": {
        "id": "5mN7GvtVu3Kx",
        "outputId": "55434ae5-057b-4ffe-dfa3-eaab41e720c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your GitHub Personal Access Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Add and commit changes (customize your commit message)\n",
        "!git add .\n",
        "!git commit -m \"feat: Add Initial EDA\" || echo \"No changes to commit.\"\n",
        "\n",
        "# 7. Push to GitHub\n",
        "!git push origin {BRANCH}"
      ],
      "metadata": {
        "id": "Cr23drSAwENv",
        "outputId": "b8b9934d-1142-491d-a2ae-7dc62636602f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "No changes to commit.\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show git status, which files are changed and staged\n",
        "!git status\n",
        "\n",
        "# Show last commit files changed (to check if notebook was included)\n",
        "!git show --name-only --oneline -1\n",
        "\n",
        "# Show current branch\n",
        "!git branch"
      ],
      "metadata": {
        "id": "RgsoqJNs6-57",
        "outputId": "bf0de325-d174-49e3-8ca1-1473de76289f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "\u001b[33mf00faf2\u001b[m\u001b[33m (\u001b[m\u001b[1;36mHEAD -> \u001b[m\u001b[1;32mmain\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/main\u001b[m\u001b[33m, \u001b[m\u001b[1;31morigin/HEAD\u001b[m\u001b[33m)\u001b[m Created using Colab\n",
            "notebooks/01_Exploratory_Data_Analysis.ipynb\n",
            "* \u001b[32mmain\u001b[m\n"
          ]
        }
      ]
    }
  ]
}